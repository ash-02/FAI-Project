# -*- coding: utf-8 -*-
"""sai Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iooLxrkhPvsyPO4r392uUp8hJMOK8QhX
"""

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, RepeatVector, TimeDistributed

from nltk.translate.bleu_score import sentence_bleu

data = pd.read_csv('ted_talks_en.csv', encoding='latin1', on_bad_lines='skip')
data.head()

data = data.dropna(subset=['transcript', 'description'])

train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)

print(f"Training size: {len(train_data)}, Test size: {len(test_data)}")

# Define vocabulary size and sequence lengths
vocab_size = 20000
max_transcript_len = 400
max_summary_len = 100

# Initialize the Tokenizer
tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>", filters='')
tokenizer.fit_on_texts(train_data['transcript'].values.tolist() + train_data['description'].values.tolist())

# Tokenize and pad transcripts and summaries
train_transcript_sequences = pad_sequences(
    tokenizer.texts_to_sequences(train_data['transcript']),
    maxlen=max_transcript_len,
    padding='post'
)
train_summary_sequences = pad_sequences(
    tokenizer.texts_to_sequences(train_data['description']),
    maxlen=max_summary_len,
    padding='post'
)

# Do the same for test data
test_transcript_sequences = pad_sequences(
    tokenizer.texts_to_sequences(test_data['transcript']),
    maxlen=max_transcript_len,
    padding='post'
)
test_summary_sequences = pad_sequences(
    tokenizer.texts_to_sequences(test_data['description']),
    maxlen=max_summary_len,
    padding='post'
)

print(tokenizer.word_index)  # Check vocabulary
print("Sample transcript",train_data['transcript'][:5])
print("Sample tokenized transcript:", train_transcript_sequences[:5])
print("Sample summary",train_data['description'][:5])
print("Sample tokenized summary:", train_summary_sequences[:5])

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense
from tensorflow.keras.preprocessing.sequence import pad_sequences


embedding_dim = 256
lstm_units = 512
vocab_size = 20000
max_transcript_len = 4000
max_summary_len = 100

# Encoder
encoder_inputs = Input(shape=(max_transcript_len,))
encoder_embedding = Embedding(vocab_size, embedding_dim, mask_zero=True)(encoder_inputs)
encoder_lstm = LSTM(lstm_units, return_state=True,dropout=0.3, recurrent_dropout=0.3)
_, state_h, state_c = encoder_lstm(encoder_embedding)
encoder_states = [state_h, state_c]

# Decoder
decoder_inputs = Input(shape=(max_summary_len,))
decoder_embedding = Embedding(vocab_size, embedding_dim, mask_zero=True)(decoder_inputs)
decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True,dropout=0.3, recurrent_dropout=0.3)
decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)
decoder_dense = Dense(vocab_size, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# Define the model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# # Build the model
# model = Sequential()

# # Encoder
# model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_transcript_len))
# model.add(LSTM(256))

# # Repeat vector to match summary sequence length
# model.add(RepeatVector(max_summary_len))

# # Decoder
# model.add(LSTM(256, return_sequences=True))
# model.add(TimeDistributed(Dense(vocab_size, activation='softmax')))

# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# # Train the model
batch_size = 64
epochs = 30

# # Shift summary sequences for decoder input and target data
decoder_input_data = pad_sequences(train_summary_sequences[:, :-1], maxlen=max_summary_len, padding='post')
decoder_target_data = pad_sequences(train_summary_sequences[:, 1:], maxlen=max_summary_len, padding='post')

#Train the model
history = model.fit(
    [train_transcript_sequences, decoder_input_data],
    decoder_target_data,
    batch_size=batch_size,
    epochs=epochs,
    validation_split=0.2
)

# history = model.fit(
#     train_transcript_sequences,
#     train_summary_sequences,
#     batch_size=batch_size,
#     epochs=epochs,
#     validation_split=0.2
# )

# Prepare sample input for encoder
sample_input_seq = test_transcript_sequences[:1]  # Use a single sample input
print("Sample input sequence:", sample_input_seq)

# Initialize the decoder input with the start token
start_token = tokenizer.word_index.get("<START>", 1)  # Default to 1 if <START> token is not in the vocabulary
decoder_input_seq = np.zeros((1, max_summary_len))  # Shape (1, max_summary_len)
decoder_input_seq[0, 0] = start_token  # Set the first token to <START>

# Predict using the model
pred = model.predict([sample_input_seq, decoder_input_seq])
print("Prediction shape:", pred.shape)

# Convert predicted token indices to text
predicted_tokens = np.argmax(pred[0], axis=-1)
print("Predicted tokens:", predicted_tokens)

# Convert tokens to words
predicted_text = ' '.join([tokenizer.index_word.get(idx, "<OOV>") for idx in predicted_tokens if idx > 0])
print("Predicted text is:", predicted_text)