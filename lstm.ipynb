{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5fca2ef-342f-4f5f-8b6b-4755cb0b0e09",
   "metadata": {},
   "source": [
    "# 1. Downloading code and initial EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "66250f3d-6f2b-44ed-bbef-98468943dab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From CNN Daily Mail on Kaggle (composed of news articles with their summaries)\n",
    "# https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5a5790c-e79e-4c43-85ac-0032e0c64a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make runtimes go vroom vroom\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "99452c68-891d-495c-8c7e-0c4355754988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducbility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1646cbda-0aa7-4a77-a098-ffd40d6c4904",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filepath = 'data/cnn_dailymail/'\n",
    "\n",
    "# Observations: Why is train-test-split like 280k-14k-13k ??\n",
    "train_df = pd.read_csv(data_filepath + 'train.csv').drop(columns=['id'])\n",
    "validation_df = pd.read_csv(data_filepath + 'validation.csv').drop(columns=['id'])\n",
    "test_df = pd.read_csv(data_filepath + 'test.csv').drop(columns=['id'])\n",
    "\n",
    "train_df.rename(columns={'article': 'transcript', 'highlights': 'summary'}, inplace=True)\n",
    "validation_df.rename(columns={'article': 'transcript', 'highlights': 'summary'}, inplace=True)\n",
    "test_df.rename(columns={'article': 'transcript', 'highlights': 'summary'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6b3c59a5-4fa7-4a9f-8ee7-1a8e7cdef5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 60k samples instead for train_df, scaling size of val and test sets as well\n",
    "train_df = train_df.sample(n=60000, random_state=42).reset_index(drop=True)\n",
    "validation_df = validation_df.sample(n=round(len(validation_df) / (280 / 60)), random_state=42).reset_index(drop=True)\n",
    "test_df = test_df.sample(n=round(len(test_df) / (280 / 60)), random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "800820c2-5ecb-4fed-9613-248f5fd22899",
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_dict = {\n",
    "    \"ain't\": \"are not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2817e5ec-3018-4da1-97ef-2eb99b2a9b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   transcript  60000 non-null  object\n",
      " 1   summary     60000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 937.6+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9494a18e-1dfc-4603-a3a8-d4593a957ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commenting out for now because plan to use attention in LSTM\n",
    "# Stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(value):\n",
    "    if isinstance(value, str):\n",
    "        # Remove ('s)\n",
    "        value = re.sub(r\"'s\", '', value)\n",
    "        # Contractions expansion\n",
    "        for contraction, expansion in contractions_dict.items():\n",
    "            value = re.sub(r'\\b' + re.escape(contraction) + r'\\b', expansion, value, flags=re.IGNORECASE)\n",
    "        # Remove all punctuation and special characters\n",
    "        value = re.sub(r'[^\\w\\s]', '', value)\n",
    "        # Tokenize, remove short words\n",
    "        tokens = [word for word in word_tokenize(value.lower()) if len(word) > 2]\n",
    "        return ' '.join(tokens)\n",
    "    return value\n",
    "\n",
    "# Function to process a chunk of data\n",
    "def process_column(column):\n",
    "    return column.apply(clean_text)\n",
    "\n",
    "# Main cleaning function\n",
    "def clean_data(data):\n",
    "    num_workers = 16 \n",
    "    \n",
    "    # Parallelize 'transcript' cleaning\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        transcript_chunks = list(tqdm(\n",
    "            executor.map(process_column, np.array_split(data['transcript'], num_workers)),\n",
    "            total=num_workers,\n",
    "            desc=\"Cleaning 'transcript'\"\n",
    "        ))\n",
    "    data['transcript_cleaned'] = pd.concat(transcript_chunks, ignore_index=True)\n",
    "\n",
    "    # Parallelize 'summary' cleaning\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        summary_chunks = list(tqdm(\n",
    "            executor.map(process_column, np.array_split(data['summary'], num_workers)),\n",
    "            total=num_workers,\n",
    "            desc=\"Cleaning 'summary'\"\n",
    "        ))\n",
    "    data['summary_cleaned'] = pd.concat(summary_chunks, ignore_index=True)\n",
    "\n",
    "    # Drop rows where cleaned columns are null or empty\n",
    "    data = data.dropna(subset=['summary_cleaned', 'transcript_cleaned'])\n",
    "    data = data[\n",
    "        ~data['summary_cleaned'].str.strip().eq('') &\n",
    "        ~data['transcript_cleaned'].str.strip().eq('')\n",
    "    ]\n",
    "\n",
    "    # Retain only cleaned columns\n",
    "    data = data[['transcript_cleaned', 'summary_cleaned']].rename(\n",
    "        columns={'transcript_cleaned': 'transcript', 'summary_cleaned': 'summary'}\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2845132b-4ddd-4593-9098-18a2ef1073fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bubbles/miniconda3/envs/lstm/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "Cleaning 'transcript': 100%|█████████████████████████████████████████████████████████████████████████████| 16/16 [00:13<00:00,  1.15it/s]\n",
      "/home/bubbles/miniconda3/envs/lstm/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "Cleaning 'summary': 100%|████████████████████████████████████████████████████████████████████████████████| 16/16 [00:01<00:00, 10.82it/s]\n",
      "/home/bubbles/miniconda3/envs/lstm/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "Cleaning 'transcript': 100%|█████████████████████████████████████████████████████████████████████████████| 16/16 [00:02<00:00,  7.73it/s]\n",
      "/home/bubbles/miniconda3/envs/lstm/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "Cleaning 'summary': 100%|███████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 134.96it/s]\n",
      "/home/bubbles/miniconda3/envs/lstm/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "Cleaning 'transcript': 100%|█████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 18.61it/s]\n",
      "/home/bubbles/miniconda3/envs/lstm/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "Cleaning 'summary': 100%|███████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 155.27it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df = clean_data(train_df)\n",
    "validation_df = clean_data(validation_df)\n",
    "test_df = clean_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "31b9012c-be41-4b2c-8006-951cc10f956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add START and END tokens to summaries\n",
    "train_df['summary'] = train_df['summary'].apply(lambda x: '<sos> ' + x + ' <eos>')\n",
    "validation_df['summary'] = validation_df['summary'].apply(lambda x: '<sos> ' + x + ' <eos>')\n",
    "test_df['summary'] = test_df['summary'].apply(lambda x: '<sos> ' + x + ' <eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ff5767ad-4501-4c39-8d2f-ea19e4d33b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.iloc[42]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9df096fc-94eb-45ee-b917-4335c8c2bbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# EDA on sentence length for helping us choose max seq length when adding padding\n",
    "transcript_word_count = []\n",
    "summary_word_count = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in train_df['transcript']:\n",
    "      transcript_word_count.append(len(i.split()))\n",
    "\n",
    "for i in train_df['summary']:\n",
    "      summary_word_count.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'transcript':transcript_word_count, 'summary':summary_word_count})\n",
    "length_df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e8fb6c25-3a43-4c29-a369-255b7c0f8ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on this let's choose max seq lengths that capture most of the distribution in the graphs above\n",
    "max_len_summary = 100\n",
    "max_len_transcript = 1000\n",
    "vocab_size = 20_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534d8c6b-55c0-44e5-b342-0dad65b06272",
   "metadata": {},
   "source": [
    "# 2. Time to make tokenizers woohoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c6f654d5-a528-45b4-9319-fea44a69429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, num_words=None, oov_token=\"<unk>\"):\n",
    "        self.word_index = {\"<pad>\": 0, oov_token: 1}\n",
    "        self.index_word = {0: \"<pad>\", 1: oov_token}\n",
    "        self.num_words = num_words\n",
    "        self.oov_token = oov_token\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        word_counts = Counter(word for text in texts for word in text.split())\n",
    "        sorted_vocab = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for idx, (word, _) in enumerate(sorted_vocab, start=2):\n",
    "            if self.num_words and idx >= self.num_words:\n",
    "                break\n",
    "            self.word_index[word] = idx\n",
    "            self.index_word[idx] = word\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        return [\n",
    "            [self.word_index.get(word, self.word_index[self.oov_token]) for word in text.split()]\n",
    "            for text in texts\n",
    "        ]\n",
    "\n",
    "def pad_sequences(seqs, maxlen, padding_value=0):\n",
    "    padded_seqs = []\n",
    "    for seq in seqs:\n",
    "        if len(seq) > maxlen:\n",
    "            seq = seq[:maxlen]\n",
    "        padded_seqs.append(torch.tensor(seq + [padding_value] * (maxlen - len(seq))))\n",
    "    return torch.stack(padded_seqs)\n",
    "\n",
    "def process_data(df, max_len_transcript, max_len_summary, x_tokenizer, y_tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenize and pad sequences for both transcripts and summaries.\n",
    "    Args:\n",
    "        df: DataFrame containing 'transcript' and 'summary'.\n",
    "        max_len_transcript: Maximum length for transcript sequences.\n",
    "        max_len_summary: Maximum length for summary sequences.\n",
    "        x_tokenizer: Tokenizer for transcripts.\n",
    "        y_tokenizer: Tokenizer for summaries.\n",
    "    Returns:\n",
    "        x_padded: Padded sequences for transcripts.\n",
    "        y_padded: Padded sequences for summaries.\n",
    "    \"\"\"\n",
    "    # Tokenize and pad transcripts\n",
    "    x_seq = x_tokenizer.texts_to_sequences(df['transcript'])\n",
    "    x_padded = pad_sequences(x_seq, max_len_transcript)\n",
    "\n",
    "    # Tokenize and pad summaries\n",
    "    y_seq = y_tokenizer.texts_to_sequences(df['summary'])\n",
    "    y_padded = pad_sequences(y_seq, max_len_summary)\n",
    "    \n",
    "    return x_padded, y_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3ad27b6a-4f0b-4f16-917d-6ecc35035c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: Traceback (most recent call last):\n",
      "  File \"/home/bubbles/miniconda3/envs/lstm/lib/python3.12/multiprocessing/queues.py\", line 259, in _feed\n",
      "    reader_close()\n",
      "  File \"/home/bubbles/miniconda3/envs/lstm/lib/python3.12/multiprocessing/connection.py\", line 178, in close\n",
      "    self._close()\n",
      "  File \"/home/bubbles/miniconda3/envs/lstm/lib/python3.12/multiprocessing/connection.py\", line 377, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bubbles/miniconda3/envs/lstm/lib/python3.12/multiprocessing/queues.py\", line 259, in _feed\n",
      "    reader_close()\n",
      "  File \"/home/bubbles/miniconda3/envs/lstm/lib/python3.12/multiprocessing/connection.py\", line 178, in close\n",
      "    self._close()\n",
      "  File \"/home/bubbles/miniconda3/envs/lstm/lib/python3.12/multiprocessing/connection.py\", line 377, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bubbles/miniconda3/envs/lstm/lib/python3.12/multiprocessing/queues.py\", line 259, in _feed\n",
      "    reader_close()\n",
      "  File \"/home/bubbles/miniconda3/envs/lstm/lib/python3.12/multiprocessing/connection.py\", line 178, in close\n",
      "    self._close()\n",
      "  File \"/home/bubbles/miniconda3/envs/lstm/lib/python3.12/multiprocessing/connection.py\", line 377, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "<function _ConnectionBase.__del__ at 0x7efbed7cee80>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bubbles/miniconda3/envs/lstm/lib/python3.12/multiprocessing/connection.py\", line 133, in __del__\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bubbles/miniconda3/envs/lstm/lib/python3.12/multiprocessing/queues.py\", line 259, in _feed\n",
      "    reader_close()\n",
      "  File \"/home/bubbles/miniconda3/envs/lstm/lib/python3.12/multiprocessing/connection.py\", line 178, in close\n",
      "    self._close()\n",
      "  File \"/home/bubbles/miniconda3/envs/lstm/lib/python3.12/multiprocessing/connection.py\", line 377, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "    self._close()\n",
      "  File \"/home/bubbles/miniconda3/envs/lstm/lib/python3.12/multiprocessing/connection.py\", line 377, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: X=torch.Size([60000, 1000]), Y=torch.Size([60000, 100])\n",
      "Validation Data: X=torch.Size([2865, 1000]), Y=torch.Size([2865, 100])\n",
      "Test Data: X=torch.Size([2462, 1000]), Y=torch.Size([2462, 100])\n"
     ]
    }
   ],
   "source": [
    "x_tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<unk>\")\n",
    "y_tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<unk>\")\n",
    "\n",
    "# Fit tokenizers on training set\n",
    "x_tokenizer.fit_on_texts(train_df['transcript'])\n",
    "y_tokenizer.fit_on_texts(train_df['summary'])\n",
    "\n",
    "# Process data\n",
    "x_train_padded, y_train_padded = process_data(train_df, max_len_transcript, max_len_summary, x_tokenizer, y_tokenizer)\n",
    "x_val_padded, y_val_padded = process_data(validation_df, max_len_transcript, max_len_summary, x_tokenizer, y_tokenizer)\n",
    "x_test_padded, y_test_padded = process_data(test_df, max_len_transcript, max_len_summary, x_tokenizer, y_tokenizer)\n",
    "\n",
    "# Log processed data shapes\n",
    "print(f\"Training Data: X={x_train_padded.shape}, Y={y_train_padded.shape}\")\n",
    "print(f\"Validation Data: X={x_val_padded.shape}, Y={y_val_padded.shape}\")\n",
    "print(f\"Test Data: X={x_test_padded.shape}, Y={y_test_padded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb696373-bec5-416a-a332-b87e4f22ac04",
   "metadata": {},
   "source": [
    "# 3. Actually (finally) building the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c30db763-5a04-44bd-8133-579622f0939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de3e76c-f368-4fd7-b0db-6f581b87588c",
   "metadata": {},
   "source": [
    "## Architecture and hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "33a60370-679f-4e66-b50a-558025fa7ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_layers, dropout, pretrained_embeddings):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)  # Fine-tune embeddings\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim, \n",
    "            hidden_dim, \n",
    "            num_layers, \n",
    "            dropout=dropout, \n",
    "            batch_first=True, \n",
    "            bidirectional=True \n",
    "        )\n",
    "        self.hidden_proj = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.cell_proj = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Combine bidirectional outputs for compatibility with decoder\n",
    "        hidden = torch.tanh(self.hidden_proj(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))) \n",
    "        cell = torch.tanh(self.cell_proj(torch.cat((cell[-2,:,:], cell[-1,:,:]), dim=1))) \n",
    "\n",
    "        return hidden.unsqueeze(0), cell.unsqueeze(0)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_layers, dropout, pretrained_embeddings, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)  # Fine-tune embeddings\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim, \n",
    "            hidden_dim, \n",
    "            num_layers, \n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # Final fully connected layer for predictions\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # Add a sequence dimension (batch_size -> batch_size, 1)\n",
    "        x = x.unsqueeze(1)\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        predictions = self.fc(outputs.squeeze(1))  # Remove seq dim: (batch_size, output_dim)\n",
    "        return predictions, hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c90328ef-5ccd-4753-aa20-7068dead7717",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, pad_idx=None):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Forward pass for Seq2Seq model.\n",
    "        \n",
    "        Args:\n",
    "            src: Source sequence (input) tensor of shape (batch_size, src_len).\n",
    "            trg: Target sequence (output) tensor of shape (batch_size, trg_len).\n",
    "            teacher_forcing_ratio: Probability of using teacher forcing.\n",
    "        \n",
    "        Returns:\n",
    "            outputs: Tensor of shape (batch_size, trg_len, trg_vocab_size).\n",
    "        \"\"\"\n",
    "        trg_len = trg.shape[1]\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        input_token = trg[:, 0]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
    "            outputs[:, t, :] = output \n",
    "\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            input_token = trg[:, t] if teacher_force else output.argmax(1)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e843df0-2304-4784-8ea9-215a34932a50",
   "metadata": {},
   "source": [
    "## Setup and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f23f5e9d-fca3-4223-9b94-9071efc60c35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading glove-wiki-gigaword-300 embeddings...\n",
      "Embeddings loaded successfully.\n",
      "Loading glove-wiki-gigaword-300 embeddings...\n",
      "Embeddings loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Get pretrained embeddings\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def load_pretrained_embeddings(word_index, embedding_dim=300, embedding_type='glove-wiki-gigaword-300'):\n",
    "    \"\"\"\n",
    "    Load pretrained embeddings using Gensim and create an embedding matrix.\n",
    "\n",
    "    Args:\n",
    "        word_index: Vocabulary dictionary (word -> index).\n",
    "        embedding_dim: Dimension of word vectors.\n",
    "        embedding_type: Type of pretrained embeddings to load ('glove-wiki-gigaword-300', 'fasttext-wiki-news-subwords-300', etc.).\n",
    "\n",
    "    Returns:\n",
    "        embedding_matrix: A PyTorch tensor with pretrained embeddings.\n",
    "    \"\"\"\n",
    "    print(f\"Loading {embedding_type} embeddings...\")\n",
    "    embeddings_model = api.load(embedding_type)\n",
    "    print(\"Embeddings loaded successfully.\")\n",
    "\n",
    "    # Initialize embedding matrix\n",
    "    vocab_size = len(word_index) + 1  # +1 for padding token\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    for word, idx in word_index.items():\n",
    "        if word in embeddings_model:\n",
    "            embedding_matrix[idx] = embeddings_model[word]\n",
    "        else:\n",
    "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))  # Random init for OOV words\n",
    "\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "embedding_dim = 300 \n",
    "x_pretrained_embeddings = load_pretrained_embeddings(x_tokenizer.word_index, embedding_dim, embedding_type='glove-wiki-gigaword-300')\n",
    "y_pretrained_embeddings = load_pretrained_embeddings(y_tokenizer.word_index, embedding_dim, embedding_type='glove-wiki-gigaword-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cab2cd21-9613-4812-8de3-26226159dc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 512\n",
    "embedding_dim_x = x_pretrained_embeddings.size(1)\n",
    "embedding_dim_y = y_pretrained_embeddings.size(1)\n",
    "vocab_size_x = len(x_tokenizer.word_index) + 1 \n",
    "vocab_size_y = len(y_tokenizer.word_index) + 1\n",
    "num_layers = 1\n",
    "dropout_rate = 0.5\n",
    "learning_rate = 1e-3 \n",
    "embedding_lr = 1e-4\n",
    "num_epochs = 30\n",
    "batch_size = 128 \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(\n",
    "    embed_dim=embedding_dim_x, \n",
    "    hidden_dim=hidden_dim, \n",
    "    num_layers=num_layers, \n",
    "    dropout=dropout_rate, \n",
    "    pretrained_embeddings=x_pretrained_embeddings\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    embed_dim=embedding_dim_y, \n",
    "    hidden_dim=hidden_dim, \n",
    "    num_layers=num_layers, \n",
    "    dropout=dropout_rate, \n",
    "    pretrained_embeddings=y_pretrained_embeddings,\n",
    "    output_dim=vocab_size_y\n",
    ")\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device, pad_idx=0).to(device)\n",
    "\n",
    "# Set up optimizer with differential learning rates\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': encoder.embedding.parameters(), 'lr': embedding_lr},\n",
    "    {'params': encoder.lstm.parameters()},\n",
    "    {'params': decoder.embedding.parameters(), 'lr': embedding_lr},\n",
    "    {'params': decoder.lstm.parameters()},\n",
    "    {'params': decoder.fc.parameters()}\n",
    "], lr=learning_rate)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a2c389-cf66-4d10-af0a-9e4483a524ab",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9f369a8f-148b-4010-9b58-68dc80c3b6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "def get_subset(dataset, fraction=0.2):\n",
    "    subset_size = int(len(dataset) * fraction)\n",
    "    subset_indices = random.sample(range(len(dataset)), subset_size)\n",
    "    return Subset(dataset, subset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0907beb3-1d20-4e2b-a1f5-db29a71bec60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsampled Train DataLoader: 94 batches of size 128\n",
      "Validation DataLoader: 23 batches of size 128\n",
      "Test DataLoader: 20 batches of size 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3763/143856901.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_train_padded = torch.tensor(x_train_padded, dtype=torch.long).pin_memory()\n",
      "/tmp/ipykernel_3763/143856901.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train_padded = torch.tensor(y_train_padded, dtype=torch.long).pin_memory()\n",
      "/tmp/ipykernel_3763/143856901.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_val_padded = torch.tensor(x_val_padded, dtype=torch.long).pin_memory()\n",
      "/tmp/ipykernel_3763/143856901.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_val_padded = torch.tensor(y_val_padded, dtype=torch.long).pin_memory()\n",
      "/tmp/ipykernel_3763/143856901.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_padded = torch.tensor(x_test_padded, dtype=torch.long).pin_memory()\n",
      "/tmp/ipykernel_3763/143856901.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_padded = torch.tensor(y_test_padded, dtype=torch.long).pin_memory()\n"
     ]
    }
   ],
   "source": [
    "y_train_padded = torch.tensor(y_train_padded, dtype=torch.long).pin_memory()\n",
    "x_val_padded = torch.tensor(x_val_padded, dtype=torch.long).pin_memory()\n",
    "y_val_padded = torch.tensor(y_val_padded, dtype=torch.long).pin_memory()\n",
    "x_test_padded = torch.tensor(x_test_padded, dtype=torch.long).pin_memory()\n",
    "y_test_padded = torch.tensor(y_test_padded, dtype=torch.long).pin_memory()\n",
    "\n",
    "train_dataset = TensorDataset(x_train_padded, y_train_padded)\n",
    "val_dataset = TensorDataset(x_val_padded, y_val_padded)\n",
    "test_dataset = TensorDataset(x_test_padded, y_test_padded)\n",
    "\n",
    "# Apply subsampling to the training dataset\n",
    "fraction = 0.2\n",
    "train_subset = get_subset(train_dataset, fraction=fraction)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_subset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=24,\n",
    "    prefetch_factor=8,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "print(f\"Subsampled Train DataLoader: {len(train_loader)} batches of size {batch_size}\")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    pin_memory=True, \n",
    "    num_workers=24, \n",
    "    prefetch_factor=8, \n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    pin_memory=True, \n",
    "    num_workers=24, \n",
    "    prefetch_factor=8, \n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "print(f\"Validation DataLoader: {len(val_loader)} batches of size {batch_size}\")\n",
    "print(f\"Test DataLoader: {len(test_loader)} batches of size {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5cea6a-8600-43b4-83c2-0997625c36ed",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1b650cc5-147f-4878-a8bc-6c70a9ed4ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_accuracy(predictions, targets, pad_idx=0):\n",
    "    pred_tokens = predictions.argmax(-1)\n",
    "    correct = (pred_tokens == targets).float()\n",
    "    mask = (targets != pad_idx).float()  # Ignore <pad> tokens\n",
    "    accuracy = (correct * mask).sum() / mask.sum()\n",
    "    return accuracy.item()\n",
    "\n",
    "def masked_loss(output, target, criterion, pad_idx=0):\n",
    "    mask = target != pad_idx\n",
    "    output = output.view(-1, output.size(-1))\n",
    "    target = target.view(-1)\n",
    "    loss = criterion(output, target)\n",
    "    loss = (loss * mask.view(-1)).sum() / mask.sum()\n",
    "    return loss\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, criterion, scaler, accumulation_steps=1, teacher_forcing_ratio=0.5):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    with tqdm(total=len(train_loader), desc=\"Training\", unit=\"batch\") as pbar:\n",
    "        for step, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            X_batch, y_batch = X_batch.to(device, non_blocking=True), y_batch.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "                output = model(X_batch, y_batch, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "                loss = masked_loss(output, y_batch, criterion) / accumulation_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Update weights after accumulation_steps\n",
    "            if (step + 1) % accumulation_steps == 0 or (step + 1) == len(train_loader):\n",
    "                clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            accuracy = calculate_accuracy(output, y_batch)\n",
    "            train_loss += loss.item() * accumulation_steps\n",
    "            train_accuracy += accuracy\n",
    "\n",
    "            pbar.set_postfix({\"Loss\": loss.item() * accumulation_steps, \"Accuracy\": accuracy})\n",
    "            pbar.update(1)\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader)\n",
    "    avg_accuracy = train_accuracy / len(train_loader)\n",
    "    return avg_loss, avg_accuracy\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_accuracy = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(val_loader), desc=\"Validation\", unit=\"batch\") as pbar:\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device, non_blocking=True), y_batch.to(device, non_blocking=True)\n",
    "\n",
    "                with torch.amp.autocast(\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "                    output = model(X_batch, y_batch, teacher_forcing_ratio=0)  # No teacher forcing during validation\n",
    "                    loss = masked_loss(output, y_batch, criterion)\n",
    "\n",
    "                accuracy = calculate_accuracy(output, y_batch)\n",
    "                val_loss += loss.item()\n",
    "                val_accuracy += accuracy\n",
    "\n",
    "                pbar.set_postfix({\"Loss\": loss.item(), \"Accuracy\": accuracy})\n",
    "                pbar.update(1)\n",
    "\n",
    "    avg_loss = val_loss / len(val_loader)\n",
    "    avg_accuracy = val_accuracy / len(val_loader)\n",
    "    return avg_loss, avg_accuracy\n",
    "\n",
    "def train(model, train_dataset, val_loader, optimizer, criterion, fraction=0.2):\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Create a new subset of the training dataset for this epoch\n",
    "        train_subset = get_subset(train_dataset, fraction=fraction)  # Use train_dataset, not train_loader\n",
    "        train_loader = DataLoader(\n",
    "            train_subset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            num_workers=24,\n",
    "            prefetch_factor=8,\n",
    "            persistent_workers=True\n",
    "        )\n",
    "\n",
    "        teacher_forcing_ratio = max(0.5 - epoch * 0.02, 0.1)  # Reduce teacher forcing over epochs\n",
    "        train_loss, train_accuracy = train_one_epoch(\n",
    "            model, train_loader, optimizer, criterion, scaler, accumulation_steps=2, teacher_forcing_ratio=teacher_forcing_ratio\n",
    "        )\n",
    "\n",
    "        torch.cuda.synchronize()  # Ensure training computations complete\n",
    "\n",
    "        val_loss, val_accuracy = validate_one_epoch(model, val_loader, criterion)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "\n",
    "    print(\"Evaluating on test data...\")\n",
    "    with torch.no_grad():\n",
    "        with tqdm(total=len(test_loader), desc=\"Testing\", unit=\"batch\") as pbar:\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch, y_batch = X_batch.to(device, non_blocking=True), y_batch.to(device, non_blocking=True)\n",
    "\n",
    "                with autocast(device_type=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "                    output = model(X_batch, y_batch, teacher_forcing_ratio=0)  # No teacher forcing during evaluation\n",
    "                    loss = masked_loss(output, y_batch, criterion)\n",
    "\n",
    "                accuracy = calculate_accuracy(output, y_batch)\n",
    "                test_loss += loss.item()\n",
    "                test_accuracy += accuracy\n",
    "\n",
    "                pbar.set_postfix({\"Loss\": loss.item(), \"Accuracy\": accuracy})\n",
    "                pbar.update(1)\n",
    "\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    avg_accuracy = test_accuracy / len(test_loader)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {avg_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2bf3c9c5-10b1-448b-ad3a-3a012e274415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0073e39-d49a-4454-9080-a981e40883d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3763/2232307417.py:96: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████| 94/94 [01:03<00:00,  1.49batch/s, Loss=7.83, Accuracy=0.054]\n",
      "Validation: 100%|█████████████████████████████████████████████████████████| 23/23 [00:08<00:00,  2.80batch/s, Loss=7.54, Accuracy=0.0656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8.6153, Train Accuracy: 0.0518\n",
      "Validation Loss: 7.6692, Validation Accuracy: 0.0596\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████| 94/94 [01:02<00:00,  1.50batch/s, Loss=7.59, Accuracy=0.0645]\n",
      "Validation: 100%|█████████████████████████████████████████████████████████| 23/23 [00:06<00:00,  3.49batch/s, Loss=7.44, Accuracy=0.0656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.7021, Train Accuracy: 0.0561\n",
      "Validation Loss: 7.5478, Validation Accuracy: 0.0596\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████| 94/94 [01:02<00:00,  1.49batch/s, Loss=7.61, Accuracy=0.0639]\n",
      "Validation: 100%|█████████████████████████████████████████████████████████| 23/23 [00:06<00:00,  3.45batch/s, Loss=7.42, Accuracy=0.0652]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.6393, Train Accuracy: 0.0566\n",
      "Validation Loss: 7.5284, Validation Accuracy: 0.0597\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████| 94/94 [01:02<00:00,  1.50batch/s, Loss=7.61, Accuracy=0.0642]\n",
      "Validation: 100%|█████████████████████████████████████████████████████████| 23/23 [00:06<00:00,  3.46batch/s, Loss=7.41, Accuracy=0.0665]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.6230, Train Accuracy: 0.0598\n",
      "Validation Loss: 7.5297, Validation Accuracy: 0.0600\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████| 94/94 [01:03<00:00,  1.49batch/s, Loss=7.58, Accuracy=0.0585]\n",
      "Validation: 100%|██████████████████████████████████████████████████████████| 23/23 [00:07<00:00,  3.16batch/s, Loss=7.4, Accuracy=0.0691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.6177, Train Accuracy: 0.0601\n",
      "Validation Loss: 7.5058, Validation Accuracy: 0.0619\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████| 94/94 [01:05<00:00,  1.45batch/s, Loss=7.57, Accuracy=0.0607]\n",
      "Validation: 100%|█████████████████████████████████████████████████████████| 23/23 [00:06<00:00,  3.34batch/s, Loss=7.39, Accuracy=0.0638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.6073, Train Accuracy: 0.0618\n",
      "Validation Loss: 7.4981, Validation Accuracy: 0.0600\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████| 94/94 [01:02<00:00,  1.50batch/s, Loss=7.59, Accuracy=0.0593]\n",
      "Validation: 100%|█████████████████████████████████████████████████████████| 23/23 [00:06<00:00,  3.49batch/s, Loss=7.39, Accuracy=0.0686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.5941, Train Accuracy: 0.0619\n",
      "Validation Loss: 7.4914, Validation Accuracy: 0.0613\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████| 94/94 [01:02<00:00,  1.50batch/s, Loss=7.61, Accuracy=0.0575]\n",
      "Validation: 100%|█████████████████████████████████████████████████████████| 23/23 [00:06<00:00,  3.44batch/s, Loss=7.39, Accuracy=0.0638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.5946, Train Accuracy: 0.0617\n",
      "Validation Loss: 7.4875, Validation Accuracy: 0.0617\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████| 94/94 [01:02<00:00,  1.50batch/s, Loss=7.65, Accuracy=0.0613]\n",
      "Validation: 100%|██████████████████████████████████████████████████████████| 23/23 [00:06<00:00,  3.47batch/s, Loss=7.4, Accuracy=0.0678]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.5904, Train Accuracy: 0.0616\n",
      "Validation Loss: 7.4961, Validation Accuracy: 0.0605\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████| 94/94 [01:03<00:00,  1.49batch/s, Loss=7.56, Accuracy=0.0624]\n",
      "Validation: 100%|█████████████████████████████████████████████████████████| 23/23 [00:06<00:00,  3.47batch/s, Loss=7.39, Accuracy=0.0656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.5664, Train Accuracy: 0.0627\n",
      "Validation Loss: 7.4882, Validation Accuracy: 0.0606\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████| 94/94 [01:03<00:00,  1.49batch/s, Loss=7.52, Accuracy=0.0609]\n",
      "Validation: 100%|█████████████████████████████████████████████████████████| 23/23 [00:06<00:00,  3.50batch/s, Loss=7.38, Accuracy=0.0643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.5638, Train Accuracy: 0.0621\n",
      "Validation Loss: 7.4873, Validation Accuracy: 0.0610\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████| 94/94 [01:03<00:00,  1.49batch/s, Loss=7.57, Accuracy=0.067]\n",
      "Validation: 100%|█████████████████████████████████████████████████████████| 23/23 [00:06<00:00,  3.48batch/s, Loss=7.39, Accuracy=0.0638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.5518, Train Accuracy: 0.0626\n",
      "Validation Loss: 7.4878, Validation Accuracy: 0.0616\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████| 94/94 [01:03<00:00,  1.49batch/s, Loss=7.52, Accuracy=0.064]\n",
      "Validation: 100%|█████████████████████████████████████████████████████████| 23/23 [00:06<00:00,  3.51batch/s, Loss=7.37, Accuracy=0.0647]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.5540, Train Accuracy: 0.0621\n",
      "Validation Loss: 7.4854, Validation Accuracy: 0.0614\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████| 94/94 [01:03<00:00,  1.48batch/s, Loss=7.51, Accuracy=0.0673]\n",
      "Validation: 100%|██████████████████████████████████████████████████████████| 23/23 [00:06<00:00,  3.49batch/s, Loss=7.37, Accuracy=0.066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.5383, Train Accuracy: 0.0621\n",
      "Validation Loss: 7.4793, Validation Accuracy: 0.0604\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|████████████████████████████████████████████████████████████| 94/94 [01:03<00:00,  1.49batch/s, Loss=7.55, Accuracy=0.063]\n",
      "Validation: 100%|█| 23/23 [00:06<00:00,  3.51batch/s, Loss=7.39, Acc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.5457, Train Accuracy: 0.0624\n",
      "Validation Loss: 7.4816, Validation Accuracy: 0.0604\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█| 94/94 [01:03<00:00,  1.49batch/s, Loss=7.51, Accur\n",
      "Validation: 100%|█████████████████████████████████████████████████████████| 23/23 [00:06<00:00,  3.32batch/s, Loss=7.38, Accuracy=0.0673]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.5372, Train Accuracy: 0.0614\n",
      "Validation Loss: 7.4826, Validation Accuracy: 0.0608\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|██████▎                                                    | 10/94 [00:08<00:57,  1.46batch/s, Loss=7.52, Accuracy=0.0685]"
     ]
    }
   ],
   "source": [
    "train(model, train_dataset, val_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480fb4f4-ca25-4555-893e-6cb2a14e2a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
